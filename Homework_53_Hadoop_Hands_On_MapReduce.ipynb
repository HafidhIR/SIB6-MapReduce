{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIsVT46Je21m"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "Hadoop is an open-source framework which is mainly used for storage purposes and maintaining and analyzing a large amount of data or datasets on the clusters of commodity hardware, which means it is actually a data management tool.\n",
        "\n",
        "##Hadoop mainly works on 3 different modes:\n",
        "\n",
        "###Standalone Mode\n",
        "\n",
        "Pseudo-distributed Mode\n",
        "Fully-distributed Mode\n",
        "Standalone Mode\n",
        "\n",
        "By default, Hadoop is configured to run in a non distributed mode. It runs as a single Java process. Instead of HDFS, this mode utilizes the local file system. This mode is useful for debugging and there isn't any need to configure core-site.xml, hdfs-site.xml, mapred-site.xml, masters & slaves. Stand-alone mode is usually the fastest mode in Hadoop.\n",
        "\n",
        "###Pseudo-distributed Mode\n",
        "\n",
        "Hadoop can also run on a single node in a Pseudo-distributed mode. In this mode, each daemon runs on separate java processes. In this mode custom configuration is required (core-site.xml, hdfs-site.xml, mapred-site.xml). Here HDFS is utilized for input and output. This mode of deployment is useful for testing and debugging purposes.\n",
        "\n",
        "###Fully-distributed Mode\n",
        "\n",
        "This is the production mode of Hadoop. In this mode typically one machine in the cluster is designated as NameNode and another as Resource Manager exclusively. These are masters. All other nodes act as Data Node and Node Manager. These are the slaves. Configuration parameters and environment need to be specified for Hadoop Daemons.\n",
        "\n",
        "Installing Java 8\n",
        "Hadoop is a java programming-based data processing framework\n",
        "\n",
        "OpenJDK is a development environment for building applications, applets, and components using the Java programming language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SypF2Xj6OvmD"
      },
      "source": [
        "# Installing Java 8\n",
        "Hadoop is a java programming-based data processing framework\n",
        "\n",
        "OpenJDK is a development environment for building applications, applets, and components using the Java programming language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lqfPfV3BIxlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33cc8496-fc16-47a7-83d8-85813b67ce70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.22\" 2024-01-16\n",
            "OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n",
            "openjdk version \"1.8.0_402\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_402-8u402-ga-2ubuntu1~22.04-b06)\n",
            "OpenJDK 64-Bit Server VM (build 25.402-b06, mixed mode)\n",
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n",
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n",
            "#Port 22\n",
            "#GatewayPorts no\n",
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:A4KrAZS7ECX9m45h53nJ/rIjVq36YkXJ3gqxKih/L7g root@24a6a170e046\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|.+o              |\n",
            "|oo..             |\n",
            "|..o.....         |\n",
            "|o. .o.+.         |\n",
            "|o o  B oS        |\n",
            "| +o * + o.       |\n",
            "|o. O * +         |\n",
            "|+ + @ O          |\n",
            "|.oE=.X==.        |\n",
            "+----[SHA256]-----+\n",
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDILKUSHhvWZOR+CsiYOzlQeYOMkYe2GhCwY6/rtxYpX/qmWaugdAMfpjqaiRZI\n",
            "YkmSNQyrCaZjTj8jWgBI3SgkSkvv2gLZ5BkT4e2cUClqtgqpdHo88TLnemSMcDVLsJ9UbaRL/+uXtcnJdvr9dZ9vWpKET8s3JcSn\n",
            "dn6ADMo0bDX9CkSgQAsrYn4N13psH2KsC00YCPlsJZN9rB1NC/ei9m4vWiw6k66dLcYQvx2tqWwnrkM0/3rbUywd9xWeFt6jL268\n",
            "FGh/0QkRrX3k46eUan1ubRcdqDStzqNGkxd14eWpS4mAEiW8U7UPWMyPpBhTCIIzluyE084J3Lygl2Y5ndH7PzyXllp4RIcFc3+/\n",
            "TofU/7EIzoa6G3MBDONfRhYswexVlZn1z/+1jY8+kh7uhOajZzZUlF5IyAuMnX70lq5rzbqHL02naYMCuUQD22LdgJudRketvwf3\n",
            "LhqLIrW7ci2F4eb+DcGhPnVUpokmeTMVfvl5OgDFQZtQiWt4gWc= root@24a6a170e046\n",
            "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
            " 16:09:57 up 1 min,  0 users,  load average: 2.22, 0.90, 0.33\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!java -version\n",
        "\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac\n",
        "!update-alternatives --set jps /usr/lib/jvm/java-8-openjdk-amd64/bin/jps\n",
        "!java -version\n",
        "\n",
        "#Finding the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "!apt-get install openssh-server -qq > /dev/null\n",
        "!service ssh start\n",
        "\n",
        "!grep Port /etc/ssh/sshd_config\n",
        "\n",
        "#Creating a new rsa key pair with empty password\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa\n",
        "\n",
        "# See id_rsa.pub content\n",
        "!more /root/.ssh/id_rsa.pub\n",
        "\n",
        "#Copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub > $HOME/.ssh/authorized_keys\n",
        "#Changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys\n",
        "\n",
        "#Conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime\n",
        "\n",
        "\n",
        "#Downloading Hadoop 3.2.3\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n",
        "\n",
        "#Untarring the file\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "#Removing the tar file\n",
        "!rm hadoop-3.2.3.tar.gz\n",
        "\n",
        "\n",
        "#Copying the hadoop files to user/local\n",
        "!cp -r hadoop-3.2.3/ /usr/local/\n",
        "#-r copy directories recursively\n",
        "\n",
        "#Adding JAVA_HOME directory to hadoop-env.sh file\n",
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh\n",
        "\n",
        "import os\n",
        "#Creating environment variables\n",
        "#Creating Hadoop home variable\n",
        "\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += f'{os.environ[\"JAVA_HOME\"]}/bin:{os.environ[\"JRE_HOME\"]}/bin:{os.environ[\"HADOOP_HOME\"]}/sbin'\n",
        "\n",
        "#Dowloading text example to use as input\n",
        "# !wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qK_Ozh8RVza-"
      },
      "outputs": [],
      "source": [
        "#Adding required property to core-site.xlm file\n",
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/core-site.xml\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "\n",
        "<!-- Put site-specific property overrides in this file. -->\n",
        "\n",
        "<configuration>\n",
        "  <property>\n",
        "          <name>fs.defaultFS</name>\n",
        "          <value>hdfs://localhost:9000</value>\n",
        "          <description>Where HDFS NameNode can be found on the network</description>\n",
        "  </property>\n",
        "</configuration>\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iRlijZzZ5_IQ"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/hdfs-site.xml\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "\n",
        "<!-- Put site-specific property overrides in this file. -->\n",
        "\n",
        "<configuration>\n",
        "<property>\n",
        "    <name>dfs.replication</name>\n",
        "    <value>1</value>\n",
        "  </property>\n",
        "\n",
        "</configuration>\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oI7EXvcy8x5Q"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/mapred-site.xml\n",
        "<?xml version=\"1.0\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "\n",
        "<!-- Put site-specific property overrides in this file. -->\n",
        "\n",
        "<configuration>\n",
        "<property>\n",
        "    <name>mapreduce.framework.name</name>\n",
        "    <value>yarn</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>mapreduce.application.classpath</name>\n",
        "    <value>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*</value>\n",
        "  </property>\n",
        "\n",
        "</configuration>\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VXqsVh1M9Pxu"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
        "<?xml version=\"1.0\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "<configuration>\n",
        "<property>\n",
        "    <description>The hostname of the RM.</description>\n",
        "    <name>yarn.resourcemanager.hostname</name>\n",
        "    <value>localhost</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>yarn.nodemanager.aux-services</name>\n",
        "    <value>mapreduce_shuffle</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>yarn.nodemanager.env-whitelist</name>\n",
        "    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\n",
        "  </property>\n",
        "\n",
        "<!-- Site specific YARN configuration properties -->\n",
        "\n",
        "</configuration>\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeDr2ydyXYFK"
      },
      "source": [
        "# Formatting the HDFS Filesystem\n",
        "\n",
        "Before HDFS can be used for the first time the file system must be formatted. The formatting process creates an empty file system by creating the storage directories and the initial versions of the NameNodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmrjTfnQ9rzL",
        "outputId": "c4d37f96-44a0-43a7-ab1f-6c80acb0674b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /usr/local/hadoop-3.2.3/logs does not exist. Creating.\n",
            "2024-05-13 16:11:18,612 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 24a6a170e046/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.2.3\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop-3.2.3/etc/hadoop:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131; compiled by 'ubuntu' on 2022-03-20T01:18Z\n",
            "STARTUP_MSG:   java = 1.8.0_402\n",
            "************************************************************/\n",
            "2024-05-13 16:11:18,651 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2024-05-13 16:11:18,808 INFO namenode.NameNode: createNameNode [-format]\n",
            "Formatting using clusterid: CID-78bd92d6-8784-4d1c-b8b0-4eec53641d97\n",
            "2024-05-13 16:11:19,693 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2024-05-13 16:11:19,742 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2024-05-13 16:11:19,744 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2024-05-13 16:11:19,745 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2024-05-13 16:11:19,753 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
            "2024-05-13 16:11:19,753 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
            "2024-05-13 16:11:19,753 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
            "2024-05-13 16:11:19,753 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2024-05-13 16:11:19,809 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2024-05-13 16:11:19,823 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
            "2024-05-13 16:11:19,823 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2024-05-13 16:11:19,845 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2024-05-13 16:11:19,846 INFO blockmanagement.BlockManager: The block deletion will start around 2024 May 13 16:11:19\n",
            "2024-05-13 16:11:19,849 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2024-05-13 16:11:19,850 INFO util.GSet: VM type       = 64-bit\n",
            "2024-05-13 16:11:19,852 INFO util.GSet: 2.0% max memory 2.8 GB = 57.7 MB\n",
            "2024-05-13 16:11:19,852 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2024-05-13 16:11:19,867 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2024-05-13 16:11:19,867 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2024-05-13 16:11:19,874 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
            "2024-05-13 16:11:19,874 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2024-05-13 16:11:19,875 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2024-05-13 16:11:19,875 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2024-05-13 16:11:19,875 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2024-05-13 16:11:19,875 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2024-05-13 16:11:19,875 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2024-05-13 16:11:19,876 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2024-05-13 16:11:19,876 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2024-05-13 16:11:19,876 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2024-05-13 16:11:19,902 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2024-05-13 16:11:19,902 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2024-05-13 16:11:19,902 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2024-05-13 16:11:19,902 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2024-05-13 16:11:19,918 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2024-05-13 16:11:19,918 INFO util.GSet: VM type       = 64-bit\n",
            "2024-05-13 16:11:19,919 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2024-05-13 16:11:19,919 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2024-05-13 16:11:19,924 INFO namenode.FSDirectory: ACLs enabled? false\n",
            "2024-05-13 16:11:19,924 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2024-05-13 16:11:19,924 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2024-05-13 16:11:19,925 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2024-05-13 16:11:19,933 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2024-05-13 16:11:19,936 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2024-05-13 16:11:19,942 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2024-05-13 16:11:19,942 INFO util.GSet: VM type       = 64-bit\n",
            "2024-05-13 16:11:19,942 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2024-05-13 16:11:19,942 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2024-05-13 16:11:19,958 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2024-05-13 16:11:19,958 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2024-05-13 16:11:19,958 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2024-05-13 16:11:19,964 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2024-05-13 16:11:19,964 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2024-05-13 16:11:19,967 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2024-05-13 16:11:19,967 INFO util.GSet: VM type       = 64-bit\n",
            "2024-05-13 16:11:19,967 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 886.4 KB\n",
            "2024-05-13 16:11:19,967 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2024-05-13 16:11:20,010 INFO namenode.FSImage: Allocated new BlockPoolId: BP-898919462-172.28.0.12-1715616679996\n",
            "2024-05-13 16:11:20,038 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2024-05-13 16:11:20,091 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2024-05-13 16:11:20,239 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2024-05-13 16:11:20,260 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2024-05-13 16:11:20,305 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2024-05-13 16:11:20,306 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2024-05-13 16:11:20,312 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2024-05-13 16:11:20,312 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 24a6a170e046/172.28.0.12\n",
            "************************************************************/\n",
            "Starting namenodes on [localhost]\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [24a6a170e046]\n",
            "24a6a170e046: Warning: Permanently added '24a6a170e046' (ED25519) to the list of known hosts.\n",
            "nohup: ignoring input and appending output to 'nohup.out'\n",
            "2070 NodeManager\n",
            "1959 ResourceManager\n",
            "2184 Jps\n",
            "1706 SecondaryNameNode\n",
            "1514 DataNode\n",
            "1402 NameNode\n"
          ]
        }
      ],
      "source": [
        "!$HADOOP_HOME/bin/hdfs namenode -format\n",
        "\n",
        "#Creating other necessary enviroment variables before starting nodes\n",
        "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\"\n",
        "\n",
        "#Launching hdfs deamons\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh\n",
        "\n",
        "#Launching yarn deamons\n",
        "#nohup causes a process to ignore a SIGHUP signal\n",
        "!nohup $HADOOP_HOME/sbin/start-yarn.sh\n",
        "\n",
        "#Listing the running deamons\n",
        "!jps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srV27xSGXu5O"
      },
      "source": [
        "### Monitoring Hadoop cluster with hadoop admin commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUxnt00v_wqC",
        "outputId": "17631de3-b3b2-40d9-8b3d-583fd53f9798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "Present Capacity: 84016652288 (78.25 GB)\n",
            "DFS Remaining: 84016627712 (78.25 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "DFS Used%: 0.00%\n",
            "Replicated Blocks:\n",
            "\tUnder replicated blocks: 0\n",
            "\tBlocks with corrupt replicas: 0\n",
            "\tMissing blocks: 0\n",
            "\tMissing blocks (with replication factor 1): 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "Erasure Coded Block Groups: \n",
            "\tLow redundancy block groups: 0\n",
            "\tBlock groups with corrupt internal blocks: 0\n",
            "\tMissing block groups: 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "\n",
            "-------------------------------------------------\n",
            "Live datanodes (1):\n",
            "\n",
            "Name: 127.0.0.1:9866 (localhost)\n",
            "Hostname: 24a6a170e046\n",
            "Decommission Status : Normal\n",
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "Non DFS Used: 31624761344 (29.45 GB)\n",
            "DFS Remaining: 84016627712 (78.25 GB)\n",
            "DFS Used%: 0.00%\n",
            "DFS Remaining%: 72.64%\n",
            "Configured Cache Capacity: 0 (0 B)\n",
            "Cache Used: 0 (0 B)\n",
            "Cache Remaining: 0 (0 B)\n",
            "Cache Used%: 100.00%\n",
            "Cache Remaining%: 0.00%\n",
            "Xceivers: 1\n",
            "Last contact: Mon May 13 16:12:11 UTC 2024\n",
            "Last Block Report: Mon May 13 16:11:35 UTC 2024\n",
            "Num of Blocks: 0\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Report the basic file system information and statistics\n",
        "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkINc9skYBld"
      },
      "source": [
        "# MapReduce"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True: pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "Db1-dQ_gAlA0",
        "outputId": "a830ded0-8c43-4c30-f0b1-e5cea870f835"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b16dc615ea65>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input (if it has not been donwloaded yet)\n",
        "!wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt\n",
        "\n",
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count\n",
        "#Coping file from local file system to HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/101.txt /word_count\n",
        "\n",
        "#Exploring Hadoop folder\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count\n",
        "\n",
        "# Run MapReduce Example using JAVA\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount /word_count/101.txt /word_count/output/"
      ],
      "metadata": {
        "id": "-VjzSc7jRrOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea71aa9-5628-47cc-abd7-7bab96086df9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     678064 2024-05-12 14:30 /word_count/101.txt\n",
            "2024-05-12 14:30:59,052 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2024-05-12 14:30:59,675 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1715524161964_0001\n",
            "2024-05-12 14:30:59,962 WARN hdfs.DataStreamer: DataStreamer Exception\n",
            "java.nio.channels.ClosedByInterruptException\n",
            "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
            "\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:477)\n",
            "\tat org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)\n",
            "\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:141)\n",
            "\tat org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)\n",
            "\tat org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)\n",
            "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
            "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
            "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.sendPacket(DataStreamer.java:852)\n",
            "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:756)\n",
            "2024-05-12 14:31:00,046 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2024-05-12 14:31:00,212 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-05-12 14:31:00,437 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1715524161964_0001\n",
            "2024-05-12 14:31:00,439 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-12 14:31:00,706 INFO conf.Configuration: resource-types.xml not found\n",
            "2024-05-12 14:31:00,706 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2024-05-12 14:31:01,118 INFO impl.YarnClientImpl: Submitted application application_1715524161964_0001\n",
            "2024-05-12 14:31:01,206 INFO mapreduce.Job: The url to track the job: http://aaa8f0eeeb98:8088/proxy/application_1715524161964_0001/\n",
            "2024-05-12 14:31:01,207 INFO mapreduce.Job: Running job: job_1715524161964_0001\n",
            "2024-05-12 14:31:13,705 INFO mapreduce.Job: Job job_1715524161964_0001 running in uber mode : false\n",
            "2024-05-12 14:31:13,706 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2024-05-12 14:31:20,850 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-05-12 14:31:26,913 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-05-12 14:31:27,934 INFO mapreduce.Job: Job job_1715524161964_0001 completed successfully\n",
            "2024-05-12 14:31:28,072 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318005\n",
            "\t\tFILE: Number of bytes written=1108673\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=678169\n",
            "\t\tHDFS: Number of bytes written=233636\n",
            "\t\tHDFS: Number of read operations=8\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=1\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=1\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=4628\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=3660\n",
            "\t\tTotal time spent by all map tasks (ms)=4628\n",
            "\t\tTotal time spent by all reduce tasks (ms)=3660\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=4628\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=3660\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=4739072\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3747840\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=1078906\n",
            "\t\tMap output materialized bytes=318005\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=105488\n",
            "\t\tCombine output records=21438\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=318005\n",
            "\t\tReduce input records=21438\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=42876\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=187\n",
            "\t\tCPU time spent (ms)=3330\n",
            "\t\tPhysical memory (bytes) snapshot=560439296\n",
            "\t\tVirtual memory (bytes) snapshot=5081817088\n",
            "\t\tTotal committed heap usage (bytes)=615514112\n",
            "\t\tPeak Map Physical memory (bytes)=337989632\n",
            "\t\tPeak Map Virtual memory (bytes)=2539438080\n",
            "\t\tPeak Reduce Physical memory (bytes)=222449664\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2542379008\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=678064\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=233636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/output"
      ],
      "metadata": {
        "id": "k6_U5EszSUy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2de22ab-edba-4be3-8e91-db655a3b7f05"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2024-05-12 14:31 /word_count/output/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup     233636 2024-05-12 14:31 /word_count/output/part-r-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/output/part-r-00000 | head -50"
      ],
      "metadata": {
        "id": "tmhHNUEeSbyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e12ead-6232-440f-ddf7-a562cb5eb49e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"(d)\t1\n",
            "\"/H\"\t1\n",
            "\"0\"\t1\n",
            "\"02\"\t2\n",
            "\"1.\t1\n",
            "\"414\t3\n",
            "\"A\t2\n",
            "\"AT&T\t3\n",
            "\"AT&T's\t1\n",
            "\"Access\t1\n",
            "\"Acid\t3\n",
            "\"Ad-hocracy\"\t1\n",
            "\"Advanced\t1\n",
            "\"Agents\t1\n",
            "\"Al\t3\n",
            "\"All\t3\n",
            "\"American\t1\n",
            "\"An\t1\n",
            "\"And\t2\n",
            "\"Any\t1\n",
            "\"Are\t3\n",
            "\"Artificial\t1\n",
            "\"As\t1\n",
            "\"Assistant\t1\n",
            "\"Attctc\"\t2\n",
            "\"Auld\t1\n",
            "\"Autodesk,\"\t1\n",
            "\"BBS,\"\t2\n",
            "\"BIRTHPLACE\t1\n",
            "\"BRITS\t1\n",
            "\"Barry\t1\n",
            "\"Because\t1\n",
            "\"Before\t1\n",
            "\"Bell\t2\n",
            "\"Bell\"\t1\n",
            "\"BellSouth\t2\n",
            "\"Berkeley\t1\n",
            "\"Big\t1\n",
            "\"Biggest\t1\n",
            "\"Black\t3\n",
            "\"Blue\t1\n",
            "\"Bob\"\t1\n",
            "\"Bob,\t3\n",
            "\"Bob:\t1\n",
            "\"Bullet-N-Board.\"\t1\n",
            "\"Bureaucrat-ese\t1\n",
            "\"But\t4\n",
            "\"C-word.\"\t1\n",
            "\"CALIFORNIA\"\t1\n",
            "\"CC,\"\t1\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hadoop Streaming Using Python\n",
        "\n",
        "Hadoop Streaming is a feature that comes with Hadoop and allows users or developers to use various different languages for writing MapReduce programs like Python, C++, Ruby, etc.\n",
        "\n",
        "The utility will create a Map/Reduce job, submit the job to an appropriate cluster, and monitor the progress of the job until it completes."
      ],
      "metadata": {
        "id": "2Cq8XtDRTBHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring Hadoop utilities available\n",
        "!ls $HADOOP_HOME/share/hadoop/tools/lib/"
      ],
      "metadata": {
        "id": "k_Dk8vLYTDqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b03d700-da4b-418c-ab16-eaaa5027b7e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aliyun-java-sdk-core-4.5.10.jar      hadoop-gridmix-3.2.3.jar\n",
            "aliyun-java-sdk-kms-2.11.0.jar\t     hadoop-kafka-3.2.3.jar\n",
            "aliyun-java-sdk-ram-3.1.0.jar\t     hadoop-openstack-3.2.3.jar\n",
            "aliyun-sdk-oss-3.13.0.jar\t     hadoop-resourceestimator-3.2.3.jar\n",
            "aws-java-sdk-bundle-1.11.901.jar     hadoop-rumen-3.2.3.jar\n",
            "azure-data-lake-store-sdk-2.2.9.jar  hadoop-sls-3.2.3.jar\n",
            "azure-keyvault-core-1.0.0.jar\t     hadoop-streaming-3.2.3.jar\n",
            "azure-storage-7.0.0.jar\t\t     ini4j-0.5.4.jar\n",
            "hadoop-aliyun-3.2.3.jar\t\t     jdom2-2.0.6.jar\n",
            "hadoop-archive-logs-3.2.3.jar\t     kafka-clients-2.8.1.jar\n",
            "hadoop-archives-3.2.3.jar\t     lz4-java-1.7.1.jar\n",
            "hadoop-aws-3.2.3.jar\t\t     ojalgo-43.0.jar\n",
            "hadoop-azure-3.2.3.jar\t\t     opentracing-api-0.33.0.jar\n",
            "hadoop-azure-datalake-3.2.3.jar      opentracing-noop-0.33.0.jar\n",
            "hadoop-datajoin-3.2.3.jar\t     opentracing-util-0.33.0.jar\n",
            "hadoop-distcp-3.2.3.jar\t\t     org.jacoco.agent-0.8.5-runtime.jar\n",
            "hadoop-extras-3.2.3.jar\t\t     wildfly-openssl-1.0.7.Final.jar\n",
            "hadoop-fs2img-3.2.3.jar\t\t     zstd-jni-1.4.9-1.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input (if it has not been donwloaded yet)\n",
        "!wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/0/2/2.txt"
      ],
      "metadata": {
        "id": "Nkl5QNWGHmX9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count_with_python"
      ],
      "metadata": {
        "id": "f--LlKTSTHDe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/2.txt /word_count_with_python"
      ],
      "metadata": {
        "id": "JjGgo6_PTMe5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapper\n",
        "\n",
        "The mapper is an executable that reads all input records from a file/s and generates an output in the form of key-value pairs which works as input for the Reducer."
      ],
      "metadata": {
        "id": "DNYaZK6WTQgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#'#!' is known as shebang and used for interpreting the script\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "\n",
        "# reading entire line from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "  # to remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # split the line into words, output data type list\n",
        "  words = line.split()\n",
        "\n",
        "  # we are looping over the words array and printing the word\n",
        "  # with the count of 1 to the STDOUT\n",
        "  for word in words:\n",
        "    # write the results to STDOUT (standard output);\n",
        "    # what we output here will be the input for the\n",
        "    # Reduce step, i.e. the input for reducer.py\n",
        "    print('%s\\t%s' % (word, 1))"
      ],
      "metadata": {
        "id": "EJrx3ZPoTOyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3451f078-3ad1-4f74-89ba-07f34241f953"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line = \"Hi, I'm Bruce Sterling, the author of this electronic book. \"\n",
        "line = line.strip()\n",
        "words = line.split()\n",
        "for word in words:\n",
        "  data = '%s\\t%s' % (word, 1)\n",
        "\n",
        "  word, count = data.split('\\t', 1)\n",
        "  print(word, count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffbLTCcNEcF9",
        "outputId": "05132840-a2c6-4379-d48b-0c282ece81cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi, 1\n",
            "I'm 1\n",
            "Bruce 1\n",
            "Sterling, 1\n",
            "the 1\n",
            "author 1\n",
            "of 1\n",
            "this 1\n",
            "electronic 1\n",
            "book. 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducer\n",
        "\n",
        "The reducer is an executable that reads all the intermediate key-value pairs generated by the mapper and generates a final output as a result of a computation operation like addition, filtration, and aggregation.\n",
        "\n",
        "Both the mapper and the reducer read the input from stdin (line by line) and emit the output to stdout."
      ],
      "metadata": {
        "id": "BBlWq4AaTWMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "# read the entire line from STDIN\n",
        "for line in sys.stdin:\n",
        "  # remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # splitting the data on the basis of tab we have provided in mapper.py\n",
        "  word, count = line.split('\\t', 1)\n",
        "  # convert count (currently a string) to int\n",
        "  try:\n",
        "    count = int(count)\n",
        "  except ValueError:\n",
        "    # count was not a number, so silently\n",
        "    # ignore/discard this line\n",
        "    continue\n",
        "\n",
        "  # this IF-switch only works because Hadoop sorts map output\n",
        "  # by key (here: word) before it is passed to the reducer\n",
        "  if current_word == word:\n",
        "    current_count += count\n",
        "  else:\n",
        "    if current_word: #to not print current_word=None\n",
        "      # write result to STDOUT\n",
        "      print('%s\\t%s' % (current_word, current_count))\n",
        "    current_count = count\n",
        "    current_word = word\n",
        "\n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "  print('%s\\t%s' % (current_word, current_count))"
      ],
      "metadata": {
        "id": "WKWl7oZOTV2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1e8e57-ec6b-4b88-fba3-215b0aa552d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
        "!cat 2.txt | python mapper.py | sort -k1,1 | python reducer.py | head -50\n",
        "#We apply sorting after the mapper because it is the default operation in MapReduce architecture"
      ],
      "metadata": {
        "id": "DgnoB_WaTfrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd66de48-8bcb-4af4-e051-335d2b905679"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(*)\t1\n",
            "(_)\t1\n",
            "(~),\t1\n",
            "***\t2\n",
            "/\t2\n",
            "[*]\t3\n",
            "[1]\t3\n",
            "15,\t1\n",
            "1789\t1\n",
            "1791\t1\n",
            "1970's\t1\n",
            "1971**\t1\n",
            "1972\t1\n",
            "#2]\t1\n",
            "[2]\t3\n",
            "20%\t1\n",
            "(212-254-5093)\t1\n",
            "25,\t1\n",
            "2.txt\t1\n",
            "2.zip******\t1\n",
            "[3]\t2\n",
            "30\t1\n",
            "60\t1\n",
            "(72600.2026@compuserve.com);\t1\n",
            "90\t1\n",
            "a\t29\n",
            "A\t2\n",
            "ABOUT\t1\n",
            "above\t1\n",
            "abridging\t1\n",
            "accept\t1\n",
            "accepts\t1\n",
            "according\t1\n",
            "accusation;\t1\n",
            "accused\t1\n",
            "actual\t1\n",
            "addition\t1\n",
            "additional\t2\n",
            "affirmation,\t1\n",
            "Again\t1\n",
            "against\t3\n",
            "agents\t1\n",
            "agree\t2\n",
            "all\t4\n",
            "all,\t1\n",
            "all.\t1\n",
            "All\t1\n",
            "ALL\t1\n",
            "allow\t1\n",
            "already\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the files\n",
        "!chmod 777 /content/mapper.py /content/reducer.py\n",
        "#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users"
      ],
      "metadata": {
        "id": "t5k-wHESTgc3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input /word_count_with_python/2.txt \\\n",
        "  -output /word_count_with_python/output \\\n",
        "  -mapper \"python /content/mapper.py\" \\\n",
        "  -reducer \"python /content/reducer.py\""
      ],
      "metadata": {
        "id": "PHuUF1kTTj-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18acaab3-ac03-45ea-ef42-bdb4a7b4c396"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [/tmp/hadoop-unjar7821846069941967830/] [] /tmp/streamjob1753074732460576342.jar tmpDir=null\n",
            "2024-05-13 16:13:34,507 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2024-05-13 16:13:34,858 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2024-05-13 16:13:35,217 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1715616711009_0001\n",
            "2024-05-13 16:13:35,591 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-05-13 16:13:36,501 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2024-05-13 16:13:36,779 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1715616711009_0001\n",
            "2024-05-13 16:13:36,781 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-05-13 16:13:37,086 INFO conf.Configuration: resource-types.xml not found\n",
            "2024-05-13 16:13:37,086 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2024-05-13 16:13:37,618 INFO impl.YarnClientImpl: Submitted application application_1715616711009_0001\n",
            "2024-05-13 16:13:37,734 INFO mapreduce.Job: The url to track the job: http://24a6a170e046:8088/proxy/application_1715616711009_0001/\n",
            "2024-05-13 16:13:37,739 INFO mapreduce.Job: Running job: job_1715616711009_0001\n",
            "2024-05-13 16:13:50,143 INFO mapreduce.Job: Job job_1715616711009_0001 running in uber mode : false\n",
            "2024-05-13 16:13:50,144 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2024-05-13 16:14:02,352 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-05-13 16:14:10,459 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-05-13 16:14:11,481 INFO mapreduce.Job: Job job_1715616711009_0001 completed successfully\n",
            "2024-05-13 16:14:11,746 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=17609\n",
            "\t\tFILE: Number of bytes written=750594\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=15400\n",
            "\t\tHDFS: Number of bytes written=7684\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=18785\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=6180\n",
            "\t\tTotal time spent by all map tasks (ms)=18785\n",
            "\t\tTotal time spent by all reduce tasks (ms)=6180\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=18785\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=6180\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=19235840\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=6328320\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=264\n",
            "\t\tMap output records=1771\n",
            "\t\tMap output bytes=14061\n",
            "\t\tMap output materialized bytes=17615\n",
            "\t\tInput split bytes=204\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=814\n",
            "\t\tReduce shuffle bytes=17615\n",
            "\t\tReduce input records=1771\n",
            "\t\tReduce output records=814\n",
            "\t\tSpilled Records=3542\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=775\n",
            "\t\tCPU time spent (ms)=2830\n",
            "\t\tPhysical memory (bytes) snapshot=827142144\n",
            "\t\tVirtual memory (bytes) snapshot=7613235200\n",
            "\t\tTotal committed heap usage (bytes)=839385088\n",
            "\t\tPeak Map Physical memory (bytes)=328704000\n",
            "\t\tPeak Map Virtual memory (bytes)=2538520576\n",
            "\t\tPeak Reduce Physical memory (bytes)=218103808\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2538708992\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=15196\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=7684\n",
            "2024-05-13 16:14:11,747 INFO streaming.StreamJob: Output directory: /word_count_with_python/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count_with_python/output"
      ],
      "metadata": {
        "id": "C_nqizwZTmQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7f2da8-5479-45dd-e22f-ebe83bf8afb2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2024-05-13 16:14 /word_count_with_python/output/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup       7684 2024-05-13 16:14 /word_count_with_python/output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out wordcount\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count_with_python/output/part-00000"
      ],
      "metadata": {
        "id": "EKRPU4UKTogT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5cfe687-db0e-4e80-c7f6-086d2e5b2291"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"AS-IS\".\t1\n",
            "\"Defects\".\t1\n",
            "\"PROJECT\t2\n",
            "\"Project\t2\n",
            "\"Project\").\t1\n",
            "\"Right\t1\n",
            "\"Small\t6\n",
            "\"public\t1\n",
            "\"small\t1\n",
            "#2]\t1\n",
            "(*)\t1\n",
            "(212-254-5093)\t1\n",
            "(72600.2026@compuserve.com);\t1\n",
            "(_)\t1\n",
            "(and\t2\n",
            "(as\t1\n",
            "(if\t2\n",
            "(or\t3\n",
            "(such\t1\n",
            "(the\t1\n",
            "(~),\t1\n",
            "***\t2\n",
            "*******This\t1\n",
            "****The\t1\n",
            "***START**THE\t1\n",
            "**Etexts\t1\n",
            "**Welcome\t1\n",
            "*BEFORE!*\t1\n",
            "*EITHER*:\t1\n",
            "*END*THE\t1\n",
            "*These\t1\n",
            "*WANT*\t1\n",
            "*not*\t1\n",
            "/\t2\n",
            "15,\t1\n",
            "1789\t1\n",
            "1791\t1\n",
            "1970's\t1\n",
            "1971**\t1\n",
            "1972\t1\n",
            "2.txt\t1\n",
            "2.zip******\t1\n",
            "20%\t1\n",
            "25,\t1\n",
            "30\t1\n",
            "60\t1\n",
            "90\t1\n",
            "A\t2\n",
            "ABOUT\t1\n",
            "ALL\t1\n",
            "ANY\t2\n",
            "ARE\t1\n",
            "AS\t1\n",
            "ASCII\t1\n",
            "ASCII,\t1\n",
            "Again\t1\n",
            "All\t1\n",
            "Amendments\t1\n",
            "Among\t3\n",
            "Association\t3\n",
            "Attorney\t1\n",
            "B.\t1\n",
            "BE\t1\n",
            "BREACH\t1\n",
            "BUT\t2\n",
            "Benedictine\t3\n",
            "Bill\t2\n",
            "Both\t1\n",
            "But\t1\n",
            "By\t4\n",
            "CAPS,\t1\n",
            "CONSEQUENTIAL,\t1\n",
            "CONTRACT,\t1\n",
            "Charles\t1\n",
            "College\t1\n",
            "College\"\t1\n",
            "College\".\t1\n",
            "Computers,\t1\n",
            "Congress\t2\n",
            "Constitution\t1\n",
            "Constitution,\t2\n",
            "DAMAGES\t1\n",
            "DAMAGES,\t1\n",
            "DAMAGES.\t1\n",
            "DISCLAIMER\t1\n",
            "DISTRIBUTION\t1\n",
            "DOMAIN\t2\n",
            "DON'T\t1\n",
            "December\t1\n",
            "December,\t1\n",
            "Defect\t1\n",
            "Defect.\t1\n",
            "Defects\t1\n",
            "Despite\t1\n",
            "Donations*\t1\n",
            "EBCDIC\t2\n",
            "ETEXT\t3\n",
            "ETEXTS\t1\n",
            "ETEXTS**START***\t1\n",
            "ETEXTS*Ver.04.29.93*END*\t1\n",
            "EVEN\t2\n",
            "EXPRESS\t1\n",
            "Electronic\t1\n",
            "Etext\t4\n",
            "Etexts\t3\n",
            "Etexts,\t1\n",
            "Etexts.\t1\n",
            "Excessive\t1\n",
            "FITNESS\t1\n",
            "FOR\t4\n",
            "Free\t1\n",
            "GIVE\t1\n",
            "GUTENBERG\"\t1\n",
            "GUTENBERG-\t1\n",
            "GUTENBERG-TM\t1\n",
            "GUTENBERG-tm\t3\n",
            "GUTENBERG-tm\"\t1\n",
            "Government\t1\n",
            "Grand\t1\n",
            "Gutenberg\t9\n",
            "Gutenberg,\t1\n",
            "HAVE\t2\n",
            "Hart\t1\n",
            "Honor\t1\n",
            "Humans\t1\n",
            "Hundreds\t1\n",
            "I\t1\n",
            "IF\t3\n",
            "II\t1\n",
            "III\t1\n",
            "IMPLIED,\t1\n",
            "INCIDENTAL\t1\n",
            "INCLUDING\t2\n",
            "INDEMNITY\t1\n",
            "INDIRECT,\t1\n",
            "IS\t1\n",
            "IT\t1\n",
            "IV\t1\n",
            "IX\t1\n",
            "If\t6\n",
            "Illinois\t3\n",
            "In\t2\n",
            "Internet\t1\n",
            "It\t1\n",
            "Jury,\t1\n",
            "KIND,\t1\n",
            "Kramer,\t1\n",
            "LIABILITY,\t1\n",
            "LIMITED\t3\n",
            "MADE\t1\n",
            "MAY\t1\n",
            "MEDIUM\t1\n",
            "MERCHANTABILITY\t1\n",
            "MONEY\t1\n",
            "Michael\t1\n",
            "Militia,\t1\n",
            "Money\t1\n",
            "NEGLIGENCE\t1\n",
            "NO\t2\n",
            "NOT\t2\n",
            "NOTICE\t1\n",
            "No\t2\n",
            "OCR\t1\n",
            "OF\t6\n",
            "ON,\t1\n",
            "OR\t10\n",
            "OTHER\t1\n",
            "OTHERWISE\t1\n",
            "Only\t1\n",
            "Original\t1\n",
            "PARTICULAR\t1\n",
            "POSSIBILITY\t1\n",
            "PRINT!\t1\n",
            "PRINT!**FOR\t1\n",
            "PROJECT\t5\n",
            "PROVIDED\t1\n",
            "PUBLIC\t2\n",
            "PUNITIVE\t1\n",
            "PURPOSE.\t1\n",
            "Passed\t1\n",
            "Pay\t1\n",
            "Plain\t1\n",
            "Prepared\t1\n",
            "Print!\"\t6\n",
            "Professor\t1\n",
            "Project\t13\n",
            "Project's\t2\n",
            "Project,\t1\n",
            "READ\t1\n",
            "REMEDIES\t1\n",
            "Ratified\t1\n",
            "Readable\t1\n",
            "Refund\"\t1\n",
            "Replacement\t1\n",
            "Rights****\t1\n",
            "Rights.\t1\n",
            "Royalties\t1\n",
            "S.\t2\n",
            "SEND\t1\n",
            "SMALL\t2\n",
            "STRICT\t1\n",
            "SUCH\t1\n",
            "September\t1\n",
            "Since\t1\n",
            "So,\t1\n",
            "Some\t1\n",
            "Special\t1\n",
            "State\t1\n",
            "State,\t1\n",
            "States\t6\n",
            "States,\t2\n",
            "TEL:\t1\n",
            "THE\t2\n",
            "THIS\t2\n",
            "TO\t6\n",
            "TO?\t1\n",
            "Ten\t1\n",
            "Texts**\t1\n",
            "Thanks,\t1\n",
            "The\t13\n",
            "These\t1\n",
            "They\t1\n",
            "This\t2\n",
            "To\t2\n",
            "U.\t1\n",
            "UNDER\t2\n",
            "US\t1\n",
            "USE\t1\n",
            "United\t6\n",
            "V\t1\n",
            "VI\t1\n",
            "VII\t1\n",
            "VIII\t1\n",
            "Vanilla\t1\n",
            "Volunteers\t1\n",
            "WARRANTIES\t2\n",
            "WARRANTY\t1\n",
            "WARRANTY;\t1\n",
            "WHAT\t1\n",
            "War\t1\n",
            "Warrants\t1\n",
            "We\t1\n",
            "Why\t1\n",
            "World\t1\n",
            "X\t1\n",
            "YOU\t7\n",
            "You\t6\n",
            "[*]\t3\n",
            "[1]\t3\n",
            "[2]\t3\n",
            "[3]\t2\n",
            "[Etext\t1\n",
            "a\t29\n",
            "above\t1\n",
            "abridging\t1\n",
            "accept\t1\n",
            "accepts\t1\n",
            "according\t1\n",
            "accusation;\t1\n",
            "accused\t1\n",
            "actual\t1\n",
            "addition\t1\n",
            "additional\t2\n",
            "affirmation,\t1\n",
            "against\t3\n",
            "agents\t1\n",
            "agree\t2\n",
            "all\t4\n",
            "all,\t1\n",
            "all.\t1\n",
            "allow\t1\n",
            "already\t1\n",
            "also\t2\n",
            "alter\t1\n",
            "alteration,\t1\n",
            "alternatively\t2\n",
            "although\t1\n",
            "among\t1\n",
            "an\t3\n",
            "and\t39\n",
            "annual\t1\n",
            "answer\t1\n",
            "any\t12\n",
            "any)\t2\n",
            "apologies.\t1\n",
            "apologize\t1\n",
            "applicable\t1\n",
            "apply\t2\n",
            "are\t5\n",
            "arise\t1\n",
            "arising\t1\n",
            "arms,\t1\n",
            "as\t5\n",
            "ascertained\t1\n",
            "assemble,\t1\n",
            "assistance\t1\n",
            "asterisk\t1\n",
            "at\t5\n",
            "author\t1\n",
            "author,\t1\n",
            "bail\t1\n",
            "be\t29\n",
            "beagles\t1\n",
            "beagles,\t1\n",
            "bear\t1\n",
            "been\t3\n",
            "being\t1\n",
            "below,\t2\n",
            "binary,\t1\n",
            "blurb\t1\n",
            "book\t1\n",
            "but\t3\n",
            "by\t21\n",
            "calculate\t1\n",
            "calculated\t1\n",
            "can\t6\n",
            "cannot\t1\n",
            "capital,\t1\n",
            "case\t2\n",
            "case,\t1\n",
            "case.\t1\n",
            "cases\t1\n",
            "cause\t1\n",
            "cause,\t1\n",
            "cause:\t1\n",
            "certain\t1\n",
            "cessing\t1\n",
            "characters\t3\n",
            "choose\t2\n",
            "clearly\t1\n",
            "codes\t1\n",
            "combined!\t1\n",
            "committed,\t1\n",
            "common\t2\n",
            "compelled\t1\n",
            "compensation.\t1\n",
            "compiled\t1\n",
            "complicated,\t1\n",
            "compressed,\t1\n",
            "compulsory\t1\n",
            "computer\t2\n",
            "computers\t1\n",
            "confronted\t1\n",
            "consent\t1\n",
            "consequential\t1\n",
            "considerable\t1\n",
            "construed\t1\n",
            "contain\t2\n",
            "containing\t1\n",
            "content\t1\n",
            "contribution\t1\n",
            "contributions\t1\n",
            "controversy\t1\n",
            "conversion\t1\n",
            "converted\t1\n",
            "convey\t1\n",
            "copies\t3\n",
            "copy\t4\n",
            "copy.\t1\n",
            "copyright\t4\n",
            "corrupt\t1\n",
            "cost\t1\n",
            "cost,\t1\n",
            "costs\t1\n",
            "counsel\t1\n",
            "court\t1\n",
            "create\t1\n",
            "crime\t1\n",
            "crime,\t1\n",
            "criminal\t2\n",
            "cruel\t1\n",
            "damage\t1\n",
            "damaged\t1\n",
            "damages,\t2\n",
            "danger;\t1\n",
            "data,\t1\n",
            "date\t1\n",
            "days\t3\n",
            "defective\t1\n",
            "defense.\t1\n",
            "delegated\t1\n",
            "delete\t2\n",
            "deleted,\t1\n",
            "deny\t1\n",
            "deprived\t1\n",
            "derive\t2\n",
            "described\t1\n",
            "describing\t1\n",
            "didn't\t1\n",
            "directly\t1\n",
            "directors,\t1\n",
            "disclaimers\t2\n",
            "disclaims\t2\n",
            "discover\t1\n",
            "disk\t1\n",
            "disk),\t1\n",
            "disk,\t1\n",
            "disparage\t1\n",
            "displayed,\t1\n",
            "displays\t1\n",
            "distribute\t5\n",
            "distributed\t1\n",
            "distribution\t1\n",
            "district\t2\n",
            "do\t4\n",
            "does\t1\n",
            "dollars,\t1\n",
            "domain\t2\n",
            "domain\"\t1\n",
            "don't\t1\n",
            "dozen\t1\n",
            "due\t1\n",
            "due.\t1\n",
            "each\t1\n",
            "effects,\t1\n",
            "efforts\t1\n",
            "efforts,\t1\n",
            "either\t1\n",
            "electronically,\t2\n",
            "electronically.\t1\n",
            "enjoy\t1\n",
            "enumeration\t1\n",
            "equipment.\t1\n",
            "equivalent\t3\n",
            "errors,\t1\n",
            "establishment\t1\n",
            "etext\t14\n",
            "etext)\t1\n",
            "etext,\t6\n",
            "etexts\t1\n",
            "etexts,\t3\n",
            "even\t2\n",
            "every\t1\n",
            "exact\t1\n",
            "exceed\t1\n",
            "except\t1\n",
            "excessive\t1\n",
            "exclusion\t1\n",
            "exclusions\t1\n",
            "exercise\t1\n",
            "expends\t1\n",
            "expense\t1\n",
            "expense,\t2\n",
            "expenses,\t1\n",
            "explanatory\t1\n",
            "fact\t2\n",
            "fault.\t1\n",
            "favor,\t1\n",
            "fee\t2\n",
            "fees,\t2\n",
            "file\t2\n",
            "fines\t1\n",
            "following\t2\n",
            "for\t15\n",
            "forces,\t1\n",
            "form\t4\n",
            "form).\t1\n",
            "form,\t1\n",
            "forth\t1\n",
            "free\t6\n",
            "freedom\t1\n",
            "from\t7\n",
            "from.\t2\n",
            "get\t1\n",
            "give\t3\n",
            "got\t2\n",
            "gratefully\t1\n",
            "grievances.\t1\n",
            "half\t1\n",
            "harmless\t1\n",
            "has\t2\n",
            "have\t6\n",
            "header\t1\n",
            "headers\t1\n",
            "held\t1\n",
            "here?\t1\n",
            "him;\t1\n",
            "himself,\t1\n",
            "his\t2\n",
            "hold\t1\n",
            "house,\t1\n",
            "houses,\t1\n",
            "how\t1\n",
            "however,\t1\n",
            "hypertext\t2\n",
            "identify,\t1\n",
            "if\t7\n",
            "impartial\t1\n",
            "implied\t1\n",
            "imposed,\t1\n",
            "improve\t1\n",
            "in\t26\n",
            "inaccurate\t1\n",
            "including\t3\n",
            "incomplete,\t1\n",
            "indemnify\t1\n",
            "indicate\t2\n",
            "indictment\t1\n",
            "indirectly\t1\n",
            "infamous\t1\n",
            "inflicted.\t1\n",
            "informed\t1\n",
            "infringed.\t1\n",
            "infringement,\t1\n",
            "insist\t1\n",
            "instance,\t1\n",
            "intellectual\t1\n",
            "intended\t2\n",
            "into\t2\n",
            "is\t8\n",
            "issue,\t1\n",
            "it\t14\n",
            "it,\t1\n",
            "it.\t1\n",
            "its\t2\n",
            "itself,\t1\n",
            "jeopardy\t1\n",
            "jury\t3\n",
            "just\t2\n",
            "keep\t2\n",
            "know:\t1\n",
            "land\t1\n",
            "law\t1\n",
            "law,\t2\n",
            "law.\t2\n",
            "law;\t1\n",
            "lawyers\t1\n",
            "lawyers.\t1\n",
            "legal\t6\n",
            "legally\t1\n",
            "liability\t2\n",
            "liability,\t1\n",
            "liberty,\t1\n",
            "license\t1\n",
            "licenses,\t1\n",
            "life\t1\n",
            "life,\t1\n",
            "like\t1\n",
            "limb;\t1\n",
            "limitation\t1\n",
            "links;\t1\n",
            "location\t1\n",
            "long\t1\n",
            "longer,\t1\n",
            "lower\t2\n",
            "machine\t1\n",
            "machines,\t1\n",
            "make\t1\n",
            "manner\t1\n",
            "many\t1\n",
            "mark-up,\t1\n",
            "material.\t1\n",
            "may\t13\n",
            "means\t1\n",
            "medium\t3\n",
            "medium,\t2\n",
            "members\t1\n",
            "method\t1\n",
            "mh\t1\n",
            "might\t1\n",
            "militia,\t1\n",
            "modification,\t1\n",
            "modify\t1\n",
            "money\t2\n",
            "money,\t1\n",
            "more\t1\n",
            "most\t4\n",
            "must\t3\n",
            "my\t1\n",
            "named\t1\n",
            "nature\t1\n",
            "naval\t1\n",
            "necessary\t1\n",
            "net\t1\n",
            "no\t8\n",
            "nor\t8\n",
            "normal\t1\n",
            "not\t11\n",
            "not,\t1\n",
            "note\t1\n",
            "note,\t1\n",
            "now\t1\n",
            "oath\t1\n",
            "obtaining\t1\n",
            "of\t58\n",
            "of.\t1\n",
            "offense\t1\n",
            "officers,\t1\n",
            "on\t6\n",
            "one\t1\n",
            "only\t1\n",
            "opportunity\t1\n",
            "or\t42\n",
            "or:\t1\n",
            "order\t1\n",
            "original\t3\n",
            "other\t14\n",
            "others\t1\n",
            "otherwise\t2\n",
            "our\t4\n",
            "out\t1\n",
            "owner,\t1\n",
            "owns\t1\n",
            "paid\t3\n",
            "papers,\t1\n",
            "part\t2\n",
            "particularly\t1\n",
            "party\t1\n",
            "payable\t1\n",
            "paying\t1\n",
            "peace\t1\n",
            "peaceably\t1\n",
            "people\t3\n",
            "people.\t2\n",
            "periodic)\t1\n",
            "permission\t1\n",
            "person\t6\n",
            "persons\t1\n",
            "persons,\t1\n",
            "petition\t1\n",
            "physical\t2\n",
            "place\t1\n",
            "plain\t2\n",
            "post\t1\n",
            "post,\t1\n",
            "powers\t1\n",
            "prepare\t1\n",
            "prepare)\t1\n",
            "prescribed\t1\n",
            "presentment\t1\n",
            "preserved,\t1\n",
            "press,\t1\n",
            "previously\t1\n",
            "print\t1\n",
            "print!\"\t1\n",
            "private\t1\n",
            "pro-\t1\n",
            "probable\t1\n",
            "process\t2\n",
            "processors);\t1\n",
            "produced\t1\n",
            "profits\t1\n",
            "profits,\t1\n",
            "program\t1\n",
            "prohibited\t1\n",
            "prohibiting\t1\n",
            "proofread\t1\n",
            "property\t2\n",
            "property,\t1\n",
            "proprietary\t2\n",
            "prosecutions,\t1\n",
            "protection\t1\n",
            "provide\t1\n",
            "provide,\t1\n",
            "provisions\t1\n",
            "public\t5\n",
            "punctuation\t1\n",
            "punishments\t1\n",
            "put\t1\n",
            "quartered\t1\n",
            "ratios\t1\n",
            "re-examined\t1\n",
            "read\t1\n",
            "readable\t1\n",
            "readable,\t1\n",
            "reader\t1\n",
            "readily\t1\n",
            "reading\t1\n",
            "receive\t4\n",
            "received\t4\n",
            "receiving\t2\n",
            "redress\t1\n",
            "references\t1\n",
            "refund\t3\n",
            "religion,\t1\n",
            "remove,\t1\n",
            "replacement\t2\n",
            "request\t2\n",
            "request.\t1\n",
            "required\t2\n",
            "requires\t1\n",
            "reserved\t1\n",
            "respecting\t1\n",
            "respectively,\t1\n",
            "rest\t2\n",
            "resulting\t1\n",
            "retained\t1\n",
            "return\t2\n",
            "return.\t1\n",
            "right\t5\n",
            "rights,\t1\n",
            "rights.\t1\n",
            "royalties.\t1\n",
            "royalty\t2\n",
            "rules\t1\n",
            "rules,\t1\n",
            "same\t1\n",
            "scanning\t1\n",
            "searched,\t1\n",
            "searches\t1\n",
            "second\t1\n",
            "secure\t1\n",
            "security\t1\n",
            "seized.\t1\n",
            "seizures,\t1\n",
            "sending\t2\n",
            "service\t1\n",
            "set\t1\n",
            "shall\t16\n",
            "shall,\t1\n",
            "should\t2\n",
            "small\t1\n",
            "so\t3\n",
            "so,\t1\n",
            "software,\t2\n",
            "soldier\t1\n",
            "someone\t1\n",
            "something\t1\n",
            "sort\t1\n",
            "speech,\t1\n",
            "speedy\t1\n",
            "statement\t2\n",
            "statement.\t3\n",
            "states\t1\n",
            "subject\t1\n",
            "such\t2\n",
            "sue\t1\n",
            "suits\t1\n",
            "supported\t1\n",
            "take\t1\n",
            "taken\t1\n",
            "tax\t1\n",
            "taxes.\t1\n",
            "tell\t1\n",
            "tells\t1\n",
            "than\t4\n",
            "that\t9\n",
            "the\t83\n",
            "their\t1\n",
            "them\t1\n",
            "then\t1\n",
            "there\t2\n",
            "there.\t1\n",
            "thereof;\t1\n",
            "these\t2\n",
            "they\t1\n",
            "things\t1\n",
            "things,\t4\n",
            "think\t1\n",
            "this\t23\n",
            "those\t1\n",
            "through\t1\n",
            "tilde\t1\n",
            "time\t4\n",
            "time,\t1\n",
            "tm\t1\n",
            "to\t48\n",
            "to.\t1\n",
            "trademark\t1\n",
            "trademark.\t1\n",
            "transcribe\t1\n",
            "transcription\t1\n",
            "trial\t1\n",
            "trial,\t1\n",
            "tried\t1\n",
            "twenty\t1\n",
            "twice\t2\n",
            "under\t1\n",
            "underline\t1\n",
            "understand,\t1\n",
            "unless\t1\n",
            "unreasonable\t1\n",
            "unusual\t1\n",
            "upon\t1\n",
            "us\t2\n",
            "us,\t1\n",
            "use\t2\n",
            "used\t3\n",
            "using\t2\n",
            "value\t1\n",
            "violated,\t1\n",
            "virus,\t1\n",
            "want\t1\n",
            "war,\t1\n",
            "warranties\t1\n",
            "way.\t1\n",
            "we\t3\n",
            "well-regulated\t1\n",
            "were\t2\n",
            "what's\t1\n",
            "when\t2\n",
            "where\t1\n",
            "wherein\t1\n",
            "which\t1\n",
            "whom\t1\n",
            "will\t2\n",
            "wish\t1\n",
            "wish,\t1\n",
            "with\t5\n",
            "within\t4\n",
            "without\t5\n",
            "witness\t1\n",
            "witnesses\t2\n",
            "word\t2\n",
            "work\t1\n",
            "work,\t2\n",
            "works.\t1\n",
            "world\t1\n",
            "wrong\t2\n",
            "you\t36\n",
            "you!)\t1\n",
            "you,\t1\n",
            "you.\t1\n",
            "your\t6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -copyToLocal /word_count_with_python/output/part-00000 /content/hdfs-wordcount.txt"
      ],
      "metadata": {
        "id": "7wrU43Da5rrW"
      },
      "execution_count": 18,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}